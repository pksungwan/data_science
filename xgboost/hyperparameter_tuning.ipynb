{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning XGBoost\n",
    "\n",
    "For some time now I've been wondering what is the default search space used by PyCaret's `tune_model` for XGBoost model. I understand that no one search space yields the best result for all scenarios, but I've gotten some pretty good results when I used PyCaret's `tune_model` with `search_library = 'optuna'`, so naturally I wanted to know the search space used.\n",
    "\n",
    "Today, I finally figured this out using the help of [perplexity.ai](https://www.perplexity.ai/)!\n",
    "\n",
    "Here's the search space from the [source](https://github.com/pycaret/pycaret/blob/97649adf8965fd02831c14982a48323b6ce7de4c/pycaret/containers/models/classification.py#L1230-L1240):\n",
    "\n",
    "```Python\n",
    "{'learning_rate': UniformDistribution(lower=1e-06, upper=0.5, log=True),\n",
    " 'n_estimators': IntUniformDistribution(lower=10, upper=300, log=False),\n",
    " 'subsample': UniformDistribution(lower=0.2, upper=1, log=False),\n",
    " 'max_depth': IntUniformDistribution(lower=1, upper=11, log=False),\n",
    " 'colsample_bytree': UniformDistribution(lower=0.5, upper=1, log=False),\n",
    " 'min_child_weight': IntUniformDistribution(lower=1, upper=4, log=False),\n",
    " 'reg_alpha': UniformDistribution(lower=1e-10, upper=10, log=True),\n",
    " 'reg_lambda': UniformDistribution(lower=1e-10, upper=10, log=True),\n",
    " 'scale_pos_weight': UniformDistribution(lower=1, upper=50, log=False)}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sungwankim/data_science/xgboost/xgb/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-01-25 23:02:38,674] A new study created in memory with name: no-name-6174a67e-993e-44d7-ba94-5a53e4e2a8f3\n",
      "[I 2025-01-25 23:02:38,861] Trial 0 finished with value: 0.6052631578947368 and parameters: {'learning_rate': 0.0013227553947418294, 'n_estimators': 212, 'subsample': 0.5075700892365237, 'max_depth': 5, 'colsample_bytree': 0.8548878710319846, 'min_child_weight': 1.4948683678333534, 'reg_alpha': 6.833867981674323e-07, 'reg_lambda': 6.550218955483345e-06, 'scale_pos_weight': 39.722840487958436}. Best is trial 0 with value: 0.6052631578947368.\n",
      "[I 2025-01-25 23:02:38,957] Trial 1 finished with value: 0.956140350877193 and parameters: {'learning_rate': 0.13872333961293243, 'n_estimators': 262, 'subsample': 0.28200106543917, 'max_depth': 5, 'colsample_bytree': 0.5148418919728771, 'min_child_weight': 1.9886273601214315, 'reg_alpha': 2.9525558718308784e-05, 'reg_lambda': 5.593456535521854e-10, 'scale_pos_weight': 26.53166114100737}. Best is trial 1 with value: 0.956140350877193.\n",
      "[I 2025-01-25 23:02:39,105] Trial 2 finished with value: 0.9649122807017544 and parameters: {'learning_rate': 0.0476769561387674, 'n_estimators': 244, 'subsample': 0.49749823079045474, 'max_depth': 7, 'colsample_bytree': 0.5798886887633423, 'min_child_weight': 3.430041859343055, 'reg_alpha': 2.0922234799437995e-06, 'reg_lambda': 1.2042849929927403e-09, 'scale_pos_weight': 42.228474492092225}. Best is trial 2 with value: 0.9649122807017544.\n",
      "[I 2025-01-25 23:02:39,158] Trial 3 finished with value: 0.631578947368421 and parameters: {'learning_rate': 1.1625918403536853e-05, 'n_estimators': 99, 'subsample': 0.24591705016639054, 'max_depth': 7, 'colsample_bytree': 0.8566339377483037, 'min_child_weight': 1.922992394439535, 'reg_alpha': 0.05496420077258388, 'reg_lambda': 1.7645039188459233, 'scale_pos_weight': 40.45369438895131}. Best is trial 2 with value: 0.9649122807017544.\n",
      "[I 2025-01-25 23:02:39,494] Trial 4 finished with value: 0.956140350877193 and parameters: {'learning_rate': 0.027989198430106072, 'n_estimators': 258, 'subsample': 0.7510053844257638, 'max_depth': 11, 'colsample_bytree': 0.7390069876898423, 'min_child_weight': 1.6623322260553053, 'reg_alpha': 5.9625455817113924e-09, 'reg_lambda': 3.0604386103058852e-09, 'scale_pos_weight': 47.1733300662067}. Best is trial 2 with value: 0.9649122807017544.\n",
      "[I 2025-01-25 23:02:39,505] Trial 5 finished with value: 0.5877192982456141 and parameters: {'learning_rate': 0.00026651268548238635, 'n_estimators': 10, 'subsample': 0.9586238351195422, 'max_depth': 7, 'colsample_bytree': 0.7646585376977717, 'min_child_weight': 3.6345172791210825, 'reg_alpha': 4.519198804510875, 'reg_lambda': 0.014664070988712505, 'scale_pos_weight': 37.32001264226758}. Best is trial 2 with value: 0.9649122807017544.\n",
      "[I 2025-01-25 23:02:39,621] Trial 6 finished with value: 0.9649122807017544 and parameters: {'learning_rate': 0.07625414987951012, 'n_estimators': 285, 'subsample': 0.6279577475739967, 'max_depth': 8, 'colsample_bytree': 0.5570677368095585, 'min_child_weight': 3.5219411842367787, 'reg_alpha': 4.672618172627951e-07, 'reg_lambda': 0.0034420269084688, 'scale_pos_weight': 21.765032559827024}. Best is trial 2 with value: 0.9649122807017544.\n",
      "[I 2025-01-25 23:02:39,715] Trial 7 finished with value: 0.6403508771929824 and parameters: {'learning_rate': 1.6989127953849975e-06, 'n_estimators': 140, 'subsample': 0.3077213133635117, 'max_depth': 10, 'colsample_bytree': 0.8085664845402398, 'min_child_weight': 3.8467663367976455, 'reg_alpha': 5.182284944111093e-07, 'reg_lambda': 2.980726156348397e-10, 'scale_pos_weight': 6.571663808188703}. Best is trial 2 with value: 0.9649122807017544.\n",
      "[I 2025-01-25 23:02:39,879] Trial 8 finished with value: 0.6754385964912281 and parameters: {'learning_rate': 2.3400465694237682e-05, 'n_estimators': 243, 'subsample': 0.2452525011204971, 'max_depth': 6, 'colsample_bytree': 0.5860339394741343, 'min_child_weight': 1.9447206341217393, 'reg_alpha': 7.036718936076849e-06, 'reg_lambda': 0.00016068875129772673, 'scale_pos_weight': 32.26144224415321}. Best is trial 2 with value: 0.9649122807017544.\n",
      "[I 2025-01-25 23:02:40,153] Trial 9 finished with value: 0.543859649122807 and parameters: {'learning_rate': 3.982828885785516e-05, 'n_estimators': 271, 'subsample': 0.819210455920271, 'max_depth': 8, 'colsample_bytree': 0.5331932720991222, 'min_child_weight': 2.4114027620418432, 'reg_alpha': 0.011450853404749514, 'reg_lambda': 0.00040381013974822905, 'scale_pos_weight': 44.01063097750068}. Best is trial 2 with value: 0.9649122807017544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.0476769561387674, 'n_estimators': 244, 'subsample': 0.49749823079045474, 'max_depth': 7, 'colsample_bytree': 0.5798886887633423, 'min_child_weight': 3.430041859343055, 'reg_alpha': 2.0922234799437995e-06, 'reg_lambda': 1.2042849929927403e-09, 'scale_pos_weight': 42.228474492092225}\n"
     ]
    }
   ],
   "source": [
    "# Here's the code to implement the same hyperparameter optimization using Optuna without using PyCaret.\n",
    "import optuna\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    X, y = load_breast_cancer(return_X_y=True)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # 2. Suggest values of the hyperparameters using a trial object.\n",
    "    params = {\n",
    "        # Boosting learning rate (xgb’s “eta”)\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 0.5, log=True),\n",
    "        # Number of gradient boosted trees.\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 300),\n",
    "        # Sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0, log=False),\n",
    "        # Maximum tree depth for base learners.\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 11, step=1),\n",
    "        # Sampling ratio of columns when constructing each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0, log=False),\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1, 4, log=False),\n",
    "        # L1 regularization weight (xgb’s alpha).\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-10, 10.0, log=True),\n",
    "        # L2 regularization weight (xgb’s lambda).\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-10, 10, log=True),\n",
    "        # Balancing of positive and negative weights.\n",
    "        \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", 1, 50, log=False),\n",
    "    }\n",
    "\n",
    "    clf = XGBClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_preds = clf.predict(X_valid)\n",
    "    accuracy = accuracy_score(y_valid, y_preds) # PyCaret uses accuracy as the default metric.\n",
    "    return accuracy\n",
    "\n",
    "# 3. Create a study object and optimize the objective function.\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# 4. Print the best parameter.\n",
    "print(study.best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
