{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning XGBoost\n",
    "\n",
    "This is lecture note [StatQuest XGBoost playlist](https://www.youtube.com/playlist?list=PLblh5JKOoLULU0irPgs1SnKO6wqVjKUsQ) on youtube.\n",
    "\n",
    "XGBoost is implementation of gradient boosting machine (GBM).\n",
    "\n",
    "XGBoost uses unique decision trees.\n",
    "\n",
    "Initial prediction is 0.5 for both regression *AND* classification.\n",
    "\n",
    "Just like other GBMs, XGBoost trains subsequent tress on the residuals.\n",
    "\n",
    "### 1. Regression\n",
    "\n",
    "XGBoost calculates similarity score (or quality score) for residuals for each node.\n",
    "$$ \\text{similarity score} = \\frac{(\\sum \\text{residuals})^2}{(\\text{number of residuals} + \\lambda)} $$\n",
    "\n",
    "where $\\lambda$ is the regularization parameter. Regularization parameter lambda has a bigger impact when the number of samples in the node is small.\n",
    "\n",
    "When residuals are similar, they do not cancel each other out and the similarity score is relatively large, and vice versa.\n",
    "\n",
    "Similarity score is used to calculate gain of the split. Make a split and calculate similarity score for each node.\n",
    "\n",
    "$$\\text{Gain} = \\text{similarity score}_\\text{left} + \\text{similarity score}_\\text{right} - \\text{similarity score}_\\text{node}$$\n",
    "\n",
    "Choose the split with biggest gain.\n",
    "\n",
    "Continue down the split (the default is 6 levels).\n",
    "\n",
    "After tree is built, prune the tree based on gain value.\n",
    "\n",
    "gamma ($\\gamma$) is the tree complexity parameter.\n",
    "\n",
    "Gain has to be greater than gamma, otherwise pruned. Start from the bottom.\n",
    "\n",
    "Setting gamma equal to zero does not equal to no pruning because lambda can make gain negative.\n",
    "\n",
    "How does this tree make prediction?\n",
    "\n",
    "We start output value for each sample.\n",
    "\n",
    "$$\\text{output value} = \\frac{\\sum \\text{residuals}}{(\\text{number of residuals} + \\lambda)}$$\n",
    "\n",
    "Notice the formula resembles similarity score, but doesn't have the squared term on numerator. **$\\lambda$ shrinks similarity score AND output value**.\n",
    "\n",
    "The final prediction is initial prediction (0.5) + learning rate * output value for the first tree + learning rate * output value for the second tree + ...\n",
    "\n",
    "where learning Rate (eta) default = 0.3.\n",
    "\n",
    "### 2. Classification\n",
    "\n",
    "Classification has slightly different similarity score.\n",
    "\n",
    "$$ \\text{similarity score} = \\frac{(\\sum \\text{residuals})^2}{\\sum[\\text{previous probability} \\cdot (1 - \\text{previous probability})] + \\lambda} $$\n",
    "\n",
    "The minimum number of residuals in each leaf is determined by **cover**.\n",
    "\n",
    "$$\\text{Cover} = \\text{denominator of the similarity score} - \\lambda$$\n",
    "$$= \\sum[\\text{previous probability} * (1- \\text{previous probability})]$$\n",
    "\n",
    "For regression, cover = number of residuals.\n",
    "\n",
    "If the cover of the leaf is less than the minimum number of cover (default 1), XGBoost will prune that leaf.\n",
    "\n",
    "By default, the minimum number of cover is 1 is equivalent to `min_child_weight` parameter of XGBoost.\n",
    "\n",
    "Output value of classification is also different.\n",
    "\n",
    "$$\\text{output value} = \\frac{\\sum \\text{residuals}}{\\sum[\\text{previous probability} * (1- \\text{previous probability})] + \\lambda}$$\n",
    "\n",
    "### 3. Math\n",
    "\n",
    "### 4. Optimizations\n",
    "\n",
    "Note: the decision to use the threshold that gives the largest gain is made without worrying about how the leaves will be split later and that means XGBoost uses greedy algorithm to build trees. In other words, since XGBoost uses a greedy algorithm, it makes a decision without looking ahead to see if it is the absolute best choice in the long term.\n",
    "\n",
    "By using a greedy algorithm, XGBoost can build a tree relatively quickly.\n",
    "\n",
    "For XGBoost, the approximate greedy algorithm means that instead of testing all possible thresholds, we only test the quantiles (default around 33 quantiles).\n",
    "\n",
    "Weighted quantile\n",
    "\n",
    "The weights are derived from the cover metric. For regression, since cover is 1 for each observation, weighted quantiles are just like normal quantiles and contain equal number of observations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
