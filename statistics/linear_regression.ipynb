{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression code from scratch\n",
    "import numpy as np\n",
    "\n",
    "class LinearRegressionV0:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iterations):\n",
    "            y_predicted = np.dot(X, self.weights) + self.bias\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.43090204, 37.72682388, -8.9644079 , -0.08918361, 42.31392336])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample data\n",
    "X = np.array([[1, 1], [2, 2], [3, 3], [4, 4]])\n",
    "y = np.array([2, 3, 4, 5])\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=5, n_features=2, noise=1, random_state=42)\n",
    "\n",
    "# Create and fit the model on sample data\n",
    "model = LinearRegressionV0(learning_rate=0.01, n_iterations=1000)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.06623392, 38.03820354, -9.359774  , -0.16495742, 42.3063024 ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create and fit the model on sample data\n",
    "model2 = LinearRegression()\n",
    "model2.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "model2.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-squared\n",
    "\n",
    "**R-squared ($R^{2}$) is the percentage of variation explained by the relationship between two variables.**\n",
    "\n",
    "$$R^{2} = \\frac{Var(mean) - Var(fit)}{Var(mean)} = \\frac{SS(mean) - SS(fit)}{SS(mean)}$$\n",
    "\n",
    "If we assume that Var(mean) â‰¥ Var(fit), this makes $R^{2} \\in [0, 1]$. This makes $R^{2}$ a percentage.\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "$R^{2}$ answers how much better the line fit the data better than the mean. R-squared = 0.6 means 60% reduction in variance once we took a feature into account.\n",
    "\n",
    "If $R^{2}$ = 0.81, then this means that there is 81% less variation around the line than the mean. In other words, the X-Y relationship accounts for 81% variation of total variation.\n",
    "\n",
    "The statistically significant R-squared was 0.9 equals the relationship between the two variables explains 90% of the variation in the data.\n",
    "\n",
    "**Relationship with Correlation R**\n",
    "\n",
    "In the case of simple linear regression with only one independent variable, $R^{2}$ is just square of correlation R. Correlation $R \\in [-1, 1]$, so the range for $R^{2}$ makes sense.\n",
    "\n",
    "For example, if R = 0.9 then R-squared = 0.81 and if R = 0.5 then R-squared = 0.25.\n",
    "\n",
    "**Why use $R^{2}$?**\n",
    "\n",
    "$R^{2}$ is more interpretable than R because $R^{2} = 0.7$ is 1.4 times as good as $R^{2} = 0.5$.\n",
    "\n",
    "### Adjusted R-squared\n",
    "\n",
    "Why use adjusted R-squared? Equations with more parameters will never make SS(fit) worse than equations with fewer parameters. This is because least squares will cause any term that makes SS(fit) worse to be multiplied by 0, in a sense, no longer exist.\n",
    "\n",
    "The more parameters we add to the equation, the more opportunities we have for random events to reduce SS(fit) and result in a better R-squared. Thus, people report an \"adjusted R-square\" value that, in essence, scales R-squared by the number of parameters.\n",
    "\n",
    "$$R^{2}_{adj} = 1 - \\frac{SS(fit) / (n - p - 1)}{SS(mean) / (n - 1)} = 1 - \\frac{(n - 1)SS(fit)}{(n - p - 1)SS(mean)} =  1 -  \\frac{n - 1}{n - p - 1} (1 - R^{2})$$\n",
    "\n",
    "### Statistical Significance\n",
    "\n",
    "If there is only two measurements, $R^{2} = 1$.\n",
    "\n",
    "The p-value for $R^{2}$ comes from something called \"F\".\n",
    "\n",
    "$$F = \\frac{\\text{The variation in Y explained by X}}{\\text{The variation in Y not explained by X}} = \\frac{SS(mean) - SS(fit) / (p_{fit} - p_{mean})}{SS(fit) / (n - p_{fit})}$$\n",
    "\n",
    "How doe we turn this number into a p-value?\n",
    "\n",
    "Generate a set of random data calculate F and repeat for many times (ex. 10,000). The p-value is number of more extreme values divided by all the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (calculated):0.9990510005866138\n",
      "R-squared (sklearn): 0.9992307969801115\n"
     ]
    }
   ],
   "source": [
    "# Write a function to calculate R^2\n",
    "def r_squared(y_true, y_pred):\n",
    "    ss_mean = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    ss_fit = np.sum((y_true - y_pred) ** 2)\n",
    "    r2 = (ss_mean - ss_fit) / ss_mean\n",
    "    return r2\n",
    "\n",
    "# Calculate R^2\n",
    "print(f\"R-squared (calculated):{r_squared(y, model.predict(X))}\")\n",
    "\n",
    "# R^2 from sklearn\n",
    "print(f\"R-squared (sklearn): {model2.score(X, y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted R-squared (calculated): 0.9981020011732276\n",
      "Adjusted R-squared (sklearn): 0.9984615939602228\n"
     ]
    }
   ],
   "source": [
    "# Write a function to calculate adjusted R^2\n",
    "def adjusted_r_squared(y_true, y_pred, n_features):\n",
    "    n_samples = len(y_true)\n",
    "    r2 = r_squared(y_true, y_pred)\n",
    "    adj_r2 = 1 - ((1 - r2) * (n_samples - 1) / (n_samples - n_features - 1))\n",
    "    return adj_r2\n",
    "\n",
    "# Calculate adjusted R-squared\n",
    "print(f\"Adjusted R-squared (calculated): {adjusted_r_squared(y, model.predict(X), X.shape[1])}\")\n",
    "\n",
    "# Adjusted R-squared does not exist in sklearn\n",
    "print(f\"Adjusted R-squared (sklearn): {adjusted_r_squared(y, model2.predict(X), X.shape[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic (calculated): 3158.224293379773\n",
      "F-statistic (model2): 3897.1406942406193\n",
      "F-statistic (sklearn): [9.24581414 5.40663939]\n",
      "p-values: [0.05583673 0.10259428]\n"
     ]
    }
   ],
   "source": [
    "# Write a function to calculate the F-statistic\n",
    "def f_statistic(y_true, y_pred, n_features):\n",
    "    ss_mean = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    ss_fit = np.sum((y_true - y_pred) ** 2)\n",
    "\n",
    "    explained_variance = (ss_mean - ss_fit) / (n_features - 1)\n",
    "    unexplained_variance = ss_fit / (len(y_true) - n_features)\n",
    "\n",
    "    f_stat = explained_variance / unexplained_variance\n",
    "    return f_stat\n",
    "\n",
    "\n",
    "# Calculate F-statistic\n",
    "print(f\"F-statistic (calculated): {f_statistic(y, model.predict(X), X.shape[1])}\")\n",
    "print(f\"F-statistic (model2): {f_statistic(y, model2.predict(X), X.shape[1])}\")\n",
    "\n",
    "# F-statistic from sklearn\n",
    "from sklearn.feature_selection import f_regression\n",
    "f_stat, p_value = f_regression(X, y)\n",
    "print(f\"F-statistic (sklearn): {f_stat}\")\n",
    "print(f\"p-values: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.999\n",
      "Model:                            OLS   Adj. R-squared:                  0.998\n",
      "Method:                 Least Squares   F-statistic:                     1299.\n",
      "Date:                Sat, 01 Feb 2025   Prob (F-statistic):           0.000769\n",
      "Time:                        23:08:55   Log-Likelihood:                -4.3640\n",
      "No. Observations:                   5   AIC:                             14.73\n",
      "Df Residuals:                       2   BIC:                             13.56\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.8504      0.528     -1.611      0.248      -3.121       1.421\n",
      "x1            18.8061      0.618     30.428      0.001      16.147      21.465\n",
      "x2            17.5362      0.696     25.199      0.002      14.542      20.530\n",
      "==============================================================================\n",
      "Omnibus:                          nan   Durbin-Watson:                   2.330\n",
      "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.377\n",
      "Skew:                          -0.309   Prob(JB):                        0.828\n",
      "Kurtosis:                       1.806   Cond. No.                         2.52\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sungwankim/data_science/statistics/.venv/lib/python3.11/site-packages/statsmodels/stats/stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 5 samples were given.\n",
      "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n"
     ]
    }
   ],
   "source": [
    "# Using statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
